{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Proceso de entrenamiento de Zephyr 7B Beta\n",
        "Este proceso de entrenamiento ha sido realizado con Google Colab PRO, en el entorno de ejecución llamado A100 GPU, el cual contiene GPUs con la funcionalidad CUDA. Si no se cuenta con este entorno, es posible que este notebook no funcione con el entorno que tengas actualmente."
      ],
      "metadata": {
        "id": "exf_KnqkTxp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Librerías\n",
        "Primero, es necesario instalar las librerías y paquetes necesarios para este proceso:\n",
        "*   **transformers**: Proporciona herramientas para cargar, configurar y trabajar con modelos de procesamiento de lenguaje, facilitando tanto la inferencia como el entrenamiento.\n",
        "*   **peft**: Sus siglas significan Parameter Efficient Fine-Tuning, y tal como su nombre indica, permite realizar fine-tuning eficiente en modelos grandes con técnicas como LoRA (Low-Rank Adaptation). Es clave para ajustar modelos grandes reduciendo el consumo de memoria y los tiempos de cómputo.\n",
        "*   **torch**: Es el marco fundamental para el cálculo numérico y las redes neuronales en este entorno. Proporciona soporte para realizar operaciones tensoriales en la GPU.\n",
        "*   **bitsandbytes**: Es una biblioteca especializada en optimizar el manejo de modelos en formatos de precisión reducida (como 4-bit y 8-bit), permitiendo que los modelos sean más ligeros y manejables.\n",
        "*   **accelerate**: Proporciona herramientas para facilitar la ejecución en múltiples dispositivos (GPUs, TPUs, etc.) y optimizar la capacitación distribuida. Simplifica la configuración de modelos grandes para aprovechar al máximo los recursos de hardware.\n",
        "*   **huggingface_hub**: Proporciona acceso al repositorio de modelos y datasets de Hugging Face mediante un token personal para autenticarse. Se utiliza para descargar modelos preentrenados y para guardar o compartir modelos ajustados.\n",
        "*   **trl**: Es un paquete necesario para poder acceder a SFTTrainer (Supervised Fine-Tuning Trainer), una herramienta que simplifica el entrenamiento supervisado de modelos grandes, integrando directamente configuraciones como LoRA, datasets personalizados y optimización de hiperparámetros.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Dwbeqh_T8KU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXxw9SMEXzDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1430ec67-6056-42c2-862e-19ac64385b6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Collecting trl\n",
            "  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting datasets>=2.21.0 (from trl)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.21.0->trl)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Collecting xxhash (from datasets>=2.21.0->trl)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.21.0->trl)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.21.0->trl) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.21.0->trl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, bitsandbytes, datasets, trl\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.45.0 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 trl-0.13.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft torch bitsandbytes accelerate huggingface_hub trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciar sesión en Hugging-Face\n",
        "Este paso no es obligatorio, ya que principalmente sirve para subir el modelo fine-tuneado a tu repositorio. En caso de hacerlo, es necesario configurar un secret, ya que escribir el token y subir el código público, puede significar una brecha de seguridad para tu cuenta, ya que cualquiera podría acceder a ella con tu token.\n",
        "\n",
        "Para hacerlo, en el panel situado a la izquierda de la pantalla, habrá un icono de una llave. Al darle click, podemos darle a \"Añadir nuevo secreto\", marcar la opción de \"Acceso desde el cuaderno\", llamarlo hf_token y asignarle el valor de nuestro token."
      ],
      "metadata": {
        "id": "aaBz_IAnYBOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkXHk4Hor-2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6894e0f-2ab8-4afd-88ae-4f6ecf7a689a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `All App` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `All App`\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('hf_token')\n",
        "!huggingface-cli login --token {hf_token}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar el modelo\n",
        "Aquí, organizamos las librerías que vamos a necesitar para el proceso de entrenamiento, y cargamos el modelo Zephyr 7B Beta, configurando unos parámetros para que el modelo se cargue de manera eficiente, optimizando el uso de memoria y computación del hardware disponible,a la vez que se adapta a las necesidades del fine-tuning."
      ],
      "metadata": {
        "id": "iSnktEjKbCG-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znjgiO4AXtoC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "ab60c7cea29d462aa75bb730ce11f750",
            "eb6ee71b97e642558f7f6903d335285b",
            "21039639d149416db4741b3cd54b7f27",
            "2d328342e6664f7ca0ef31420bce4911",
            "f18a1bbb76b049c2a1aa3e02be684369",
            "d3709d0c8e4241f89b9f411cfbe350e0",
            "3e51d8a214ea4d9cac422c0b947042b0",
            "962dd6e798c74355a092724b971b73f2",
            "57a7b7dc6d4f49038c74b1ddf1050ee9",
            "5c57afe3f6a642efa2ac25487f94501f",
            "02d84bf01bd84b299cca24b1c7b5dc41"
          ]
        },
        "outputId": "fe0b8687-c2f4-4634-a797-7b942e942892"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   100%|          | 8/8 [00:06<00:00, 1.28it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab60c7cea29d462aa75bb730ce11f750"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import torch\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=BitsAndBytesConfig(load_in_4bit=True,                                 # Carga el modelo en un formato de 4 bits para reducir su tamaño y consumo de memoria\n",
        "                                           bnb_4bit_compute_dtype=getattr(torch, \"float16\"),  #Define que los cálculos se realizarán en precisión de 16 bits para equilibrar precisión y eficiencia\n",
        "                                           bnb_4bit_quant_type=\"nf4\"),                        # Especifica el tipo de cuantización \"nf4\" que sirve para mejorar la estabilidad numérica y la precisión en comparación con los esquemas de cuantización estándar\n",
        "    torch_dtype=torch.bfloat16, # Define que el modelo usará el tipo de dato bfloat16 (un formato más eficiente que FP32) para manejar tensores. bfloat16 es útil para modelos grandes porque reduce el uso de memoria mientras mantiene suficiente precisión\n",
        "    device_map=\"auto\",          # Permite que Hugging Face gestione automáticamente la asignación de las distintas partes del modelo entre dispositivos disponibles (por ejemplo, múltiples GPUs)\n",
        "    trust_remote_code=True,     # Habilita la descarga de código específico para el modelo directamente desde Hugging Face Hub\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de los hiperparámetros para el entrenamiento\n",
        "Esta celda configura y prepara tanto el modelo como el tokenizador para el proceso de fine-tuning. Se ajusta el modelo para entrenamiento eficiente, se define la configuración de entrenamiento y se ajustan los hiperparámetros relevantes. Aquí se establece el entorno necesario para realizar un ajuste fino supervisado con un enfoque eficiente en recursos."
      ],
      "metadata": {
        "id": "PpXHsW6geE0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfFehrEfiKRF"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache = False          # Desactiva el uso de caché, que es útil para inferencia pero innecesario en entrenamiento.\n",
        "model.config.pretraining_tp = 1         # Ajusta la cantidad de tensor parallelism usada para preentrenamiento, configurándolo para una sola división.\n",
        "model.gradient_checkpointing_disable()  # Desactiva el almacenamiento de gradientes intermedios para ahorrar memoria, ya que no se utiliza en este caso.\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) # Carga el tokenizador preentrenado del modelo Zephyr 7B desde Hugging Face Hub.\n",
        "tokenizer.padding_side = 'right'                                              # Configura el padding del tokenizador en el lado derecho de las entradas.\n",
        "tokenizer.pad_token = tokenizer.eos_token                                     # Define el token de padding como el token de fin de secuencia (EOS).\n",
        "\n",
        "tokenizer.add_eos_token = True                                                # Asegura que las secuencias se completen con un token EOS.\n",
        "tokenizer.add_bos_token, tokenizer.add_eos_token                              # Configura que se pueda incluir BOS (inicio de secuencia) y EOS (fin de secuencia).\n",
        "\n",
        "model = prepare_model_for_kbit_training(model) # Ajusta el modelo para entrenar usando cuantización en bits más bajos para optimizar la memoria y el uso de hardware.\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                 # Dimensión interna de las matrices LoRA. Un valor más alto permite que el modelo capture patrones más complejos, pero aumenta el uso de memoria y cómputo.\n",
        "    lora_alpha=16,                        # Escala de las actualizaciones de LoRA. Un valor más alto amplifica las actualizaciones, lo que puede ser útil para ajustar modelos grandes con datos complejos, pero también podría llevar a un sobreajuste.\n",
        "    lora_dropout=0.1,                     # Tasa de dropout para regularización durante entrenamiento. Un valor más alto reduce el riesgo de sobreajuste, pero puede dificultar que el modelo aprenda patrones específicos.\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Aplica LoRA solo a módulos seleccionados (proyecciones de consulta y valores del modelo).\n",
        "    task_type=\"CAUSAL_LM\"                 # Especifica que el modelo es para tareas de lenguaje causal.\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config) # Integra la configuración LoRA en el modelo\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",                # Directorio donde se guardarán los resultados y checkpoints.\n",
        "    save_steps=500,                        # Guarda un checkpoint cada 500 pasos de entrenamiento.\n",
        "    logging_steps=50,                      # Muestra logs de progreso cada 50 pasos.\n",
        "    eval_strategy=\"steps\",                 # Realiza evaluación periódica basada en pasos.\n",
        "    per_device_train_batch_size=8,         # Tamaño del batch por dispositivo (GPU).\n",
        "    gradient_accumulation_steps=1,         # Pasos de acumulación de gradientes antes de actualización.\n",
        "    learning_rate=2e-5,                    # Tasa de aprendizaje inicial para el optimizador.\n",
        "    weight_decay=0.01,                     # Decaimiento del peso (regularización).\n",
        "    optim=\"paged_adamw_32bit\",             # Optimizador AdamW paginado con precisión de 32 bits.\n",
        "    num_train_epochs=2,                    # Número de épocas completas para entrenamiento.\n",
        "    save_total_limit=3,                    # Mantiene un máximo de 3 checkpoints guardados.\n",
        "    fp16=False,                            # Deshabilita precisión mixta de 16 bits (no requerida aquí).\n",
        "    bf16=True,                             # Habilita precisión mixta de tipo bfloat16 (recomendado en GPUs modernas). !! Cambiar este parametro a false y fp16 a true en caso de no utilizar Google Colab PRO.\n",
        "    max_grad_norm=1.0,                     # Límite de norma para los gradientes, evitando explosiones de gradientes.\n",
        "    max_steps=-1,                          # Ignorado, ya que el número de pasos lo controla `num_train_epochs`.\n",
        "    warmup_ratio=0.1,                      # Proporción del calentamiento para el optimizador (10% del total).\n",
        "    group_by_length=True,                  # Agrupa datos por longitud para mejorar la eficiencia de memoria.\n",
        "    lr_scheduler_type=\"linear\",            # Usa un decaimiento lineal para la tasa de aprendizaje.\n",
        "    report_to=\"wandb\"                      # Reporta métricas de entrenamiento al sistema de monitoreo Weights & Biases (wandb).\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento, formateo y preparación de los datos\n",
        "Esta celda realiza el procesamiento, formateo y preparación de los datasets necesarios para el entrenamiento y evaluación del modelo."
      ],
      "metadata": {
        "id": "OCWIzEDjflk3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E00dMBoM-BLI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a758c7e2-358f-42ed-dcae-86abc0e20019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conjunto de entrenamiento: 2859 muestras\n",
            "Conjunto de evaluación: 505 muestras\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Función para formatear un dataset general\n",
        "def format_general_dataset(sample):\n",
        "    \"\"\"Preserva la estructura del dataset sin agregar redundancias.\"\"\"\n",
        "    try:\n",
        "        system_prompt = sample[\"<|system|>\"]\n",
        "        user_prompt = sample[\"<|user|>\"]\n",
        "        assistant_response = sample[\"<|assistant|>\"]\n",
        "\n",
        "        # Reestructurar para una clave unificada sin alterar el contenido\n",
        "        sample[\"text\"] = f\"<|system|>: {system_prompt}\\n<|user|>: {user_prompt}\\n<|assistant|>: {assistant_response}\"\n",
        "    except KeyError as e:\n",
        "        raise ValueError(f\"Falta una clave requerida en el dataset: {e}\")\n",
        "\n",
        "    return sample\n",
        "\n",
        "# Función para formatear un dataset de recomendaciones\n",
        "def format_recommendations_dataset(sample):\n",
        "    \"\"\"Formatea el dataset inicial de recomendaciones.\"\"\"\n",
        "    try:\n",
        "        system_prompt = sample[\"<|system|>\"]\n",
        "        user_data = json.loads(sample[\"<|user|>\"])[\"user_profile\"]\n",
        "        assistant_response = sample[\"<|assistant|>\"]\n",
        "\n",
        "        # Formatear el perfil del usuario en texto claro\n",
        "        user_profile = (\n",
        "            f\"User Profile:\\n\"\n",
        "            f\"- Age: {user_data.get('edad')}\\n\"\n",
        "            f\"- Monthly Income: {user_data.get('ingresos_mensuales')}\\n\"\n",
        "            f\"- Monthly Savings: {user_data.get('ahorro_mensual')}\\n\"\n",
        "            f\"- Risk Tolerance: {user_data.get('tolerancia_riesgo')}\\n\"\n",
        "            f\"- Investment Horizon: {user_data.get('horizonte_inversion')}\\n\"\n",
        "            f\"- Financial Goal: {user_data.get('objetivo_financiero')}\\n\"\n",
        "        )\n",
        "\n",
        "        # Generar texto formateado para el modelo\n",
        "        sample[\"text\"] = f\"<|system|>: {system_prompt}\\n<|user|>: {user_profile}\\n<|assistant|>: {assistant_response}\"\n",
        "    except (KeyError, json.JSONDecodeError) as e:\n",
        "        raise ValueError(f\"Error al procesar el dataset `recomendaciones_iniciales`: {e}\")\n",
        "\n",
        "    return sample\n",
        "\n",
        "# Definición de archivos de datasets\n",
        "datasets = {\n",
        "    \"comparaciones_esquematico\": os.path.join(\"q&a_comparaciones_esquematico.jsonl\"),\n",
        "    \"recomendaciones_iniciales\": os.path.join(\"recomendaciones_iniciales.jsonl\"),\n",
        "    \"comparaciones_conversacionales\": os.path.join(\"q&a_comparaciones_conversacionales.jsonl\"),\n",
        "    \"conceptos_inversiones\": os.path.join(\"conceptos_inversiones_Q&A.jsonl\")\n",
        "}\n",
        "\n",
        "# Procesar datasets con el formateo correspondiente\n",
        "processed_datasets = {}\n",
        "for name, path in datasets.items():\n",
        "    dataset = load_dataset(\"json\", data_files=path)[\"train\"]\n",
        "    if name == \"recomendaciones_iniciales\":\n",
        "        formatted_dataset = dataset.map(format_recommendations_dataset, remove_columns=[\"<|system|>\", \"<|user|>\", \"<|assistant|>\"])\n",
        "    else:\n",
        "        formatted_dataset = dataset.map(format_general_dataset, remove_columns=[\"<|system|>\", \"<|user|>\", \"<|assistant|>\"])\n",
        "\n",
        "    processed_datasets[name] = formatted_dataset\n",
        "\n",
        "# Combinar datasets\n",
        "combined_dataset = concatenate_datasets([processed_datasets[name] for name in processed_datasets])\n",
        "\n",
        "# Dividir en entrenamiento y evaluación\n",
        "final_dataset = combined_dataset.train_test_split(test_size=0.15, seed=42)\n",
        "train_dataset = final_dataset[\"train\"]\n",
        "eval_dataset = final_dataset[\"test\"]\n",
        "\n",
        "# Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # False porque es un modelo causal, no de modelado de lenguaje enmascarado\n",
        ")\n",
        "\n",
        "print(f\"Conjunto de entrenamiento: {len(train_dataset)} muestras\")\n",
        "print(f\"Conjunto de evaluación: {len(eval_dataset)} muestras\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del trainer\n",
        "Este bloque configura el trainer para realizar el fine-tune supervisado del modelo usando SFTTrainer. Este entrenador simplifica el proceso de entrenamiento supervisado, integrando configuraciones específicas como LoRA para reducir los requisitos de memoria."
      ],
      "metadata": {
        "id": "u9aX8bCwinJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3BRI3L9dqXM"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from accelerate import Accelerator\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,                     # El modelo cargado y configurado para ajuste fino (incluye LoRA y cuantización).\n",
        "    args = training_args,              # Argumentos de configuración del entrenamiento definidos previamente.\n",
        "    train_dataset = train_dataset,     # Dataset de entrenamiento\n",
        "    eval_dataset = eval_dataset,       # Dataset de evaluación para validar el desempeño del modelo durante el entrenamiento.\n",
        "    peft_config = lora_config,         # Configuración de LoRA para ajuste fino eficiente.\n",
        "    processing_class = tokenizer,      # Tokenizador asociado al modelo, responsable de preparar los textos de entrada.\n",
        "    data_collator = data_collator,     # Collator para agrupar datos, asegurando que tengan formato adecuado para el modelo.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración de WandB\n",
        "Este bloque configura Weights & Biases (WandB), que es una herramienta de seguimiento y visualización de experimentos de machine learning. Primero se autentica con una clave de acceso (wandb.login), y luego inicializa un nuevo experimento (wandb.init) con un nombre de proyecto (Fine tuning Zephyr 7B), tipo de tarea (training), y permite que los datos se registren de forma anónima. Esto permite monitorear métricas como pérdida, precisión y otros detalles del entrenamiento en tiempo real."
      ],
      "metadata": {
        "id": "kCEMuFsDxDoL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di_AikToI-QT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "f3cbd1e2-f739-4324-b332-bdb114c060dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mebayego\u001b[0m (\u001b[33mebayego-universitat-oberta-de-catalunya\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241221_153508-wa75hk1c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B/runs/wa75hk1c' target=\"_blank\">lyric-durian-7</a></strong> to <a href='https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B' target=\"_blank\">https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B/runs/wa75hk1c' target=\"_blank\">https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B/runs/wa75hk1c</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import wandb\n",
        "wandb_token = userdata.get('wandb_token')\n",
        "wandb.login(key = wandb_token)\n",
        "run = wandb.init(\n",
        "    project='Fine tuning Zephyr 7B',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo\n",
        "A continuación, se entrena el modelo con todos los parámetros y datos definidos en los bloques anteriores."
      ],
      "metadata": {
        "id": "a64ktEj1xZYF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MuPjujHGptJl",
        "outputId": "7e64582a-226f-467f-b0c4-9751363f1ee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='716' max='716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [716/716 39:45, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.635000</td>\n",
              "      <td>1.496082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.286700</td>\n",
              "      <td>0.987415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.734800</td>\n",
              "      <td>0.517745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.424300</td>\n",
              "      <td>0.370512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.312300</td>\n",
              "      <td>0.276465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.258800</td>\n",
              "      <td>0.239283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.231300</td>\n",
              "      <td>0.216503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.170100</td>\n",
              "      <td>0.200281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.182800</td>\n",
              "      <td>0.193877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.186046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.184700</td>\n",
              "      <td>0.184386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.164400</td>\n",
              "      <td>0.181378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.170200</td>\n",
              "      <td>0.178891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.176300</td>\n",
              "      <td>0.178946</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=716, training_loss=0.43346010439888727, metrics={'train_runtime': 2388.8506, 'train_samples_per_second': 2.394, 'train_steps_per_second': 0.3, 'total_flos': 1.9163662773178368e+17, 'train_loss': 0.43346010439888727, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el modelo\n",
        "Una vez ha finalizado el entrenamiento, hay que guardar el modelo para poder utilizarlo en futuras ocasiones. Este guardado, al utilizar el modelo el enfoque de LoRA que realiza ajustes en capas específicas del modelo base, al guardar el modelo, por defecto, solo se almacenan los parámetros de las capas ajustadas (el adaptador LoRA).\n",
        "\n",
        "Además, con la función 'trainer.model.push_to_hub' subimos el modelo a nuestro repositorio de Hugging Face."
      ],
      "metadata": {
        "id": "27_J71nHxhjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817,
          "referenced_widgets": [
            "344c0d5411604acdb9c97b06efb0ffae",
            "b3b9d9c3a8b84ae29032f8245041fdb6",
            "8247dada9c4a46b59dc7da9a34ef2261",
            "c97eccca84d941648870bc0e4044e7da",
            "bc3917063ce9468aa4adc34f5f0a02fd",
            "f71b86daf3c640588fdb0e05a789cab7",
            "fd556626f812485e9b986236bc303e0f",
            "5f015bc8e46343f3ab99878210048b18",
            "f91942b04ef2457989c583ee99bccdea",
            "853aa6e74e5a491c8b7d612c100ec88d",
            "412eba01a7fd406b9b42247fbd2cde40"
          ]
        },
        "id": "1yw567qxrlst",
        "outputId": "9f16d5dc-f04f-462b-fe20-87615bb1407d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▆▇▃▃▆▇▇█▄▇▁▄▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▃▂▆▆▃▂▂▁▅▂█▅▅▄</td></tr><tr><td>eval/steps_per_second</td><td>▃▂▆▆▂▂▂▁▅▂█▅▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▄▅▄▄▄█▁▂▂▂▂▂▂</td></tr><tr><td>train/learning_rate</td><td>▆█▇▇▆▆▅▄▄▃▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.17895</td></tr><tr><td>eval/runtime</td><td>47.5255</td></tr><tr><td>eval/samples_per_second</td><td>10.626</td></tr><tr><td>eval/steps_per_second</td><td>1.347</td></tr><tr><td>total_flos</td><td>1.9163662773178368e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>716</td></tr><tr><td>train/grad_norm</td><td>1.32777</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1763</td></tr><tr><td>train_loss</td><td>0.43346</td></tr><tr><td>train_runtime</td><td>2388.8506</td></tr><tr><td>train_samples_per_second</td><td>2.394</td></tr><tr><td>train_steps_per_second</td><td>0.3</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lyric-durian-7</strong> at: <a href='https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B/runs/wa75hk1c' target=\"_blank\">https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B/runs/wa75hk1c</a><br> View project at: <a href='https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B' target=\"_blank\">https://wandb.ai/ebayego-universitat-oberta-de-catalunya/Fine%20tuning%20Zephyr%207B</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241221_153508-wa75hk1c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "344c0d5411604acdb9c97b06efb0ffae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning completado. Modelo y tokenizador guardados en './final_model'.\n"
          ]
        }
      ],
      "source": [
        "new_model = \"TFG\"\n",
        "trainer.model.save_pretrained(new_model)\n",
        "wandb.finish()\n",
        "\n",
        "trainer.model.push_to_hub(new_model, use_temp_dir=False)\n",
        "\n",
        "print(\"Fine-tuning completado. Modelo y tokenizador guardados en './final_model'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardado completo del modelo\n",
        "También es posible guardar todas las capas del modelo en vez de solo las modificadas con LoRA, ya que al importar el modelo nuevamente para realizarle inferencia, será necesario primero descargar el modelo base y luego aplicarle las capas modificadas guardadas en el bloque anterior. Guardando el modelo de esta forma, nos ahorramos el paso anterior."
      ],
      "metadata": {
        "id": "3eT1JWmrydIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbSDoNntBfwn",
        "outputId": "e7b1a73a-e084-45d5-f96b-e472e5169dea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo completo guardado en './final_full_model'.\n"
          ]
        }
      ],
      "source": [
        "model = model.merge_and_unload()\n",
        "\n",
        "# Guardar el modelo completo\n",
        "output_dir = \"./final_full_model\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"Modelo completo guardado en '{output_dir}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencia del modelo\n",
        "Este bloque crea un pipeline de generación de texto usando la biblioteca transformers. El pipeline es una interfaz simplificada para generar texto con el modelo fine-tuneado. Los parámetros controlan aspectos como la longitud de las respuestas, la aleatoriedad y la diversidad de las generaciones, adaptándolo a tareas como responder preguntas o generar texto coherente."
      ],
      "metadata": {
        "id": "2-z6FvB0zAv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUYezZhCfEIW",
        "outputId": "9431f196-3853-4425-8723-1522d2fc7283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-generation\",         # Especifica la tarea de generación de texto.\n",
        "    model=model,                    # Modelo fine-tuneado que se usará para generar texto.\n",
        "    tokenizer=tokenizer,            # Tokenizador asociado al modelo, para procesar las entradas y salidas.\n",
        "    max_length=1000,                # Longitud máxima del texto generado para evitar respuestas excesivamente largas.\n",
        "    min_length=100                  # Longitud mínima para asegurar que las respuestas no se trunquen demasiado.\n",
        "    temperature=0.9,                # Controla la aleatoriedad del texto generado. Valores altos (cercanos a 1) producen respuestas más variadas.\n",
        "    top_p=0.95,                     # Ajusta la diversidad de las respuestas usando \"nucleus sampling\". Limita las palabras generadas a un subconjunto con el 95% de probabilidad acumulada.\n",
        "    eos_token_id=tokenizer.eos_token_id # Detiene la generación al encontrar el token de fin de secuencia (EOS).\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruebas de preguntas\n",
        "Los siguientes bloques, contienen prompts de ejemplo y el output que calcula el modelo para cada uno. Estas respuestas, ya se puede observar que utilizan parte del conocimiento adquirido gracias al fine-tuning."
      ],
      "metadata": {
        "id": "kZDc3w40zd3a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFve0QRW-NaD",
        "outputId": "575166ac-093a-46da-99e2-d2857e59ecaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Las ETFs (Exchange Traded Funds) se pueden clasificar en dos tipos principales: ETFs de acumulación y ETFs de distribución. La principal diferencia entre ambos es cómo se distribuyen los ingresos y los gastos entre los accionistas.\n",
            "\n",
            "En los ETFs de acumulación, los ingresos y los gastos se acumulan en la cartera de la ETF, y se distribuyen a los accionistas en forma de capital de distribución al final del año fiscal. Esto significa que los accionistas pueden optar por reinvertir los ingresos en la ETF o retirirlos en forma de dividendos. Los ETFs de acumulación pueden ser atractivos para los inversores que buscan un rendimiento más estable y predictible, ya que los ingresos se acumulan y se distribuyen en forma de capital de distribución, lo que puede reducir la volatilidad de los ingresos.\n",
            "\n",
            "En los ETFs de distribución, los ingresos y los gastos se distribuyen a los accionistas en forma de dividendos mensuales o trimestrales. Esto significa que los accionistas pueden optar por reinvertir los dividendos en la ETF o retirirlos en forma de ingresos. Los ETFs de distribución pueden ser atractivos para los inversores que buscan un rendimiento más regular y predictible, ya que los dividendos se distribuyen con mayor frecuencia.\n",
            "\n",
            "En resumen, la principal diferencia entre ETFs de acumulación y ETFs de distribución es cómo se distribuyen los ingresos y los gastos entre los accionistas. Los ETFs de acumulación acumulan los ingresos y los gastos y se distribuyen en forma de capital de distribución, mientras que los ETFs de distribución distribuyen los ingresos y los gastos en forma de dividendos. El tipo de ETF que se elige depende de la estrategia de inversión y los objetivos de rendimiento del inversor.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"¿Cuál es la diferencia entre ETFs de acumulación y distribución?\"\n",
        "result = pipe(prompt)\n",
        "result_text = result[0]['generated_text']\n",
        "cleaned_text = result_text.split(\"<|assistant|>\")[-1].strip()\n",
        "print(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL9knKZF-QEd",
        "outputId": "545b2b83-1667-4b53-94ef-55b28c013bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¿Cuales son las principales diferencias entre el MSCI World y el SP&500? \n",
            "\n",
            "Aunque ambos índices son medidas de rendimiento de la bolsa, hay algunas principales diferencias entre el MSCI World y el SP&500:\n",
            "\n",
            "1. Composición: El MSCI World es un índice global que mide el desempeño de las principales empresas de 23 países desarrollados y emergentes, mientras que el SP&500 se centra en las empresas más grandes y líderes de la economía estadounidense.\n",
            "\n",
            "2. Países: El MSCI World incluye empresas de países como Alemania, Japón, Reino Unido, Francia, China, Hong Kong, Taiwán, Sudáfrica, Australia, Nueva Zelanda, Singapur, Taiwán, Corea del Sur, y otros, además de los Estados Unidos. El SP&500 solo incluye empresas de los Estados Unidos.\n",
            "\n",
            "3. Sectorial: El MSCI World tiene una distribución sectorial más diversa que el SP&500, con una mayor representación de empresas de la industria tecnológica, de la salud y de la energía renovable. El SP&500 tiene una mayor representación de empresas de la industria de la tecnología de la información, de la finanzas y de la industria de la construcción.\n",
            "\n",
            "4. Pesos: El MSCI World tiene un peso más equilibrado entre las empresas de los diferentes países, mientras que el SP&500 tiene un peso más alto en las empresas estadounidenses.\n",
            "\n",
            "5. Calculación: El MSCI World utiliza un método de pesaje de capitalización de mercado, mientras que el SP&500 utiliza un método de pesaje de mercado libre.\n",
            "\n",
            "6. Frecuencia de rebalanzamiento: El MSCI World se rebalanza semestralmente, mientras que el SP&500 se rebalanza mensualmente.\n",
            "\n",
            "7. Fecha de lanzamiento: El MSCI World fue lanzado en 1986, mientras que el SP&500 fue lanzado en 1957.\n",
            "\n",
            "8. Administración: El MSCI World es administrado por MSCI, mientras que el SP&500 es administrado por S&P Dow Jones Indices.\n",
            "\n",
            "9. Tipo de empresa: El MSCI World incluye empresas de diferentes tipos de capitalización, incluyendo empresas de capitalización pequeña y mediana, mientras que el SP&500 solo incluye empresas de capitalización grande.\n",
            "\n",
            "10. Desempeño histórico: El MSCI World ha tenido un desempeño histórico más volátil que el SP&500, debido a la mayor diversificación de países y sectores. El SP&500 ha tenido un desempeño más estable, debido a la mayor concentración en empresas estadounidenses.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"¿Cuales son las principales diferencias entre el MSCI World y el SP&500?\"\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'].replace(\"<|assistant|>\", \"\").strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shhjnn6-DBFb",
        "outputId": "261eabf5-0632-4caf-fe2f-eacc42f22ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|system|>: Eres un asistente financiero personal especializado en inversiones. Tu objetivo es ayudar a los usuarios con cualquier tema relacionado con inversiones, finanzas o economía. Para cada respuesta:\n",
            "1. Primero, busca los datos adecuados de tus conocimientos aprendidos o de las bases de datos externas disponibles.\n",
            "2. Luego, analiza cuidadosamente la consulta para dar una respuesta clara, precisa y personalizada.\n",
            "3. Mantente amable, profesional y enfocado únicamente en temas relacionados con inversiones, finanzas y economía.<|user|>: ¿En qué aspectos son diferentes UBS ETF - Factor MSCI EMU Low Volatility UCITS ETF (EUR) A-dis y Xtrackers Euro Stoxx 50 UCITS ETF 1C?<|assistant|>: \n",
            "<|assistant|>\n",
            "Las UBS ETF - Factor MSCI EMU Low Volatility UCITS ETF (EUR) A-dis y Xtrackers Euro Stoxx 50 UCITS ETF 1C son dos ETFs (Exchange Traded Funds) que se comercializan en Europa, pero presentan diferencias en sus objetivos de inversión.\n",
            "\n",
            "La UBS ETF - Factor MSCI EMU Low Volatility UCITS ETF (EUR) A-dis se centra en la baja volatilidad, lo que significa que selecciona aquellas acciones con un historial de volatilidad baja en el mercado de valores de la Unión Europea (UE). Este ETF busca reducir el riesgo de volatilidad para los inversores, lo que puede ser atractivo para aquellos que buscan un rendimiento más estable.\n",
            "\n",
            "En contraste, el Xtrackers Euro Stoxx 50 UCITS ETF 1C se centra en la composición del índice Euro Stoxx 50, que incluye las 50 principales empresas de la región económica de la UE. Este ETF ofrece una exposición directa a la economía de la UE y a las principales empresas de la región.\n",
            "\n",
            "En resumen, la UBS ETF - Factor MSCI EMU Low Volatility UCITS ETF (EUR) A-dis se centra en la baja volatilidad, mientras que el Xtrackers Euro Stoxx 50 UCITS ETF 1C se centra en la composición del índice Euro Stoxx 50. Los inversores deben evaluar sus objetivos de inversión y preferencias de riesgo para decidir cuál de estos ETFs es más adecuado para sus necesidades.\n"
          ]
        }
      ],
      "source": [
        "prompt = (\"<|system|>: Eres un asistente financiero personal especializado en inversiones. \"\n",
        "          \"Tu objetivo es ayudar a los usuarios con cualquier tema relacionado con inversiones, finanzas o economía. \"\n",
        "          \"Para cada respuesta:\\n\"\n",
        "          \"1. Primero, busca los datos adecuados de tus conocimientos aprendidos o de las bases de datos externas disponibles.\\n\"\n",
        "          \"2. Luego, analiza cuidadosamente la consulta para dar una respuesta clara, precisa y personalizada.\\n\"\n",
        "          \"3. Mantente amable, profesional y enfocado únicamente en temas relacionados con inversiones, finanzas y economía.\\n\"\n",
        "          \"<|user|>: ¿En qué aspectos son diferentes UBS ETF - Factor MSCI EMU Low Volatility UCITS ETF (EUR) A-dis y Xtrackers Euro Stoxx 50 UCITS ETF 1C?\"\n",
        "          \"<|assistant|>:\"\n",
        ")\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardado de todos los archivos\n",
        "Por último, dado que Google Colab no permite descargar varios archivos o carpetas enteras, este bloque reune todos los archivos utilizados en el proyecto en un arhivo comprimido y lo descarga para poder reutilizarlo en futuras ocasiones.\n",
        "\n",
        "Nota: la última linea (files.download(\"/content/file.zip\")) descarga automaticamente el arhivo comprimido, pero con un tiempo de espera muy elevado. Para descargar el archivo más rápidamente una vez ya generado, podemos dirigirnos al panel de la zona izquierda, clickar en el icono de la carpeta, y con botón derecho encima del archivo llamado 'file.zip', hacer click en descargar, lo cual será mucho más rápido que esperar a que el comando prepare la descarga."
      ],
      "metadata": {
        "id": "ib5g1vjWz1L6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0yFxnM553Rh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "422833d6-44ec-4c72-b7e5-a6b1fc04bc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/ (stored 0%)\n",
            "updating: content/.config/ (stored 0%)\n",
            "updating: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "updating: content/.config/default_configs.db (deflated 98%)\n",
            "updating: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "updating: content/.config/gce (stored 0%)\n",
            "updating: content/.config/config_sentinel (stored 0%)\n",
            "updating: content/.config/.last_update_check.json (deflated 22%)\n",
            "updating: content/.config/logs/ (stored 0%)\n",
            "updating: content/.config/logs/2024.12.19/ (stored 0%)\n",
            "updating: content/.config/logs/2024.12.19/14.20.18.151587.log (deflated 58%)\n",
            "updating: content/.config/logs/2024.12.19/14.20.29.520330.log (deflated 57%)\n",
            "updating: content/.config/logs/2024.12.19/14.20.16.940511.log (deflated 87%)\n",
            "updating: content/.config/logs/2024.12.19/14.19.43.316528.log (deflated 93%)\n",
            "updating: content/.config/logs/2024.12.19/14.20.30.129972.log (deflated 57%)\n",
            "updating: content/.config/logs/2024.12.19/14.20.05.781718.log (deflated 58%)\n",
            "updating: content/.config/configurations/ (stored 0%)\n",
            "updating: content/.config/configurations/config_default (deflated 15%)\n",
            "updating: content/.config/active_config (stored 0%)\n",
            "updating: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "updating: content/TFG/ (stored 0%)\n",
            "updating: content/TFG/adapter_config.json (deflated 53%)\n",
            "updating: content/TFG/README.md (stored 0%)\n",
            "updating: content/TFG/adapter_model.safetensors (deflated 7%)\n",
            "updating: content/conceptos_inversiones_Q&A.jsonl (deflated 84%)\n",
            "updating: content/final_full_model/ (stored 0%)\n",
            "updating: content/final_full_model/tokenizer.json (deflated 85%)\n",
            "updating: content/final_full_model/generation_config.json (deflated 20%)\n",
            "updating: content/final_full_model/tokenizer_config.json (deflated 68%)\n",
            "updating: content/final_full_model/tokenizer.model (deflated 55%)\n",
            "updating: content/final_full_model/model.safetensors (deflated 18%)\n",
            "updating: content/final_full_model/special_tokens_map.json (deflated 70%)\n",
            "updating: content/final_full_model/config.json (deflated 54%)\n",
            "updating: content/recomendaciones_iniciales.jsonl (deflated 98%)\n",
            "updating: content/wandb/ (stored 0%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/ (stored 0%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/tmp/ (stored 0%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/tmp/code/ (stored 0%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/files/ (stored 0%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/files/requirements.txt (deflated 55%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/files/wandb-summary.json (deflated 46%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/files/wandb-metadata.json (deflated 44%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/files/config.yaml (deflated 74%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/files/output.log (deflated 86%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/logs/ (stored 0%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/logs/debug-core.log (deflated 64%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/logs/debug-internal.log (deflated 74%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/logs/debug.log (deflated 68%)\n",
            "updating: content/wandb/run-20241221_142905-1fl7aonm/run-1fl7aonm.wandb (deflated 79%)\n",
            "updating: content/wandb/latest-run/ (stored 0%)\n",
            "updating: content/wandb/latest-run/tmp/ (stored 0%)\n",
            "updating: content/wandb/latest-run/tmp/code/ (stored 0%)\n",
            "updating: content/wandb/latest-run/files/ (stored 0%)\n",
            "updating: content/wandb/latest-run/files/requirements.txt (deflated 55%)\n",
            "updating: content/wandb/latest-run/files/wandb-summary.json (deflated 47%)\n",
            "updating: content/wandb/latest-run/files/wandb-metadata.json (deflated 44%)\n",
            "updating: content/wandb/latest-run/files/config.yaml (deflated 74%)\n",
            "updating: content/wandb/latest-run/files/output.log (deflated 86%)\n",
            "updating: content/wandb/latest-run/logs/ (stored 0%)\n",
            "updating: content/wandb/latest-run/logs/debug-core.log (deflated 64%)\n",
            "updating: content/wandb/latest-run/logs/debug-internal.log (deflated 74%)\n",
            "updating: content/wandb/latest-run/logs/debug.log (deflated 68%)\n",
            "updating: content/wandb/debug-internal.log (deflated 74%)\n",
            "updating: content/wandb/debug.log (deflated 68%)\n",
            "updating: content/q&a_comparaciones_esquematico.jsonl (deflated 93%)\n",
            "updating: content/results/ (stored 0%)\n",
            "updating: content/results/checkpoint-716/ (stored 0%)\n",
            "updating: content/results/checkpoint-716/adapter_config.json (deflated 53%)\n",
            "updating: content/results/checkpoint-716/tokenizer.json (deflated 85%)\n",
            "updating: content/results/checkpoint-716/scheduler.pt (deflated 56%)\n",
            "updating: content/results/checkpoint-716/trainer_state.json (deflated 77%)\n",
            "updating: content/results/checkpoint-716/optimizer.pt (deflated 8%)\n",
            "updating: content/results/checkpoint-716/README.md (deflated 66%)\n",
            "updating: content/results/checkpoint-716/tokenizer_config.json (deflated 68%)\n",
            "updating: content/results/checkpoint-716/tokenizer.model (deflated 55%)\n",
            "updating: content/results/checkpoint-716/training_args.bin (deflated 51%)\n",
            "updating: content/results/checkpoint-716/rng_state.pth (deflated 25%)\n",
            "updating: content/results/checkpoint-716/special_tokens_map.json (deflated 70%)\n",
            "updating: content/results/checkpoint-716/adapter_model.safetensors (deflated 7%)\n",
            "updating: content/results/checkpoint-500/ (stored 0%)\n",
            "updating: content/results/checkpoint-500/adapter_config.json (deflated 53%)\n",
            "updating: content/results/checkpoint-500/tokenizer.json (deflated 85%)\n",
            "updating: content/results/checkpoint-500/scheduler.pt (deflated 55%)\n",
            "updating: content/results/checkpoint-500/trainer_state.json (deflated 75%)\n",
            "updating: content/results/checkpoint-500/optimizer.pt (deflated 8%)\n",
            "updating: content/results/checkpoint-500/README.md (deflated 66%)\n",
            "updating: content/results/checkpoint-500/tokenizer_config.json (deflated 68%)\n",
            "updating: content/results/checkpoint-500/tokenizer.model (deflated 55%)\n",
            "updating: content/results/checkpoint-500/training_args.bin (deflated 51%)\n",
            "updating: content/results/checkpoint-500/rng_state.pth (deflated 25%)\n",
            "updating: content/results/checkpoint-500/special_tokens_map.json (deflated 70%)\n",
            "updating: content/results/checkpoint-500/adapter_model.safetensors (deflated 7%)\n",
            "updating: content/q&a_comparaciones_conversacionales.jsonl (deflated 93%)\n",
            "updating: content/sample_data/ (stored 0%)\n",
            "updating: content/sample_data/anscombe.json (deflated 83%)\n",
            "updating: content/sample_data/README.md (deflated 39%)\n",
            "updating: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "updating: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "updating: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "updating: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/tmp/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/files/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/files/wandb-summary.json (stored 0%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/files/wandb-metadata.json (deflated 44%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/files/config.yaml (deflated 72%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/files/output.log (deflated 39%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/logs/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/logs/debug-core.log (deflated 66%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/logs/debug-internal.log (deflated 73%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/logs/debug.log (deflated 67%)\n",
            "  adding: content/wandb/run-20241221_152515-tt5m98wg/run-tt5m98wg.wandb (stored 0%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/tmp/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/tmp/code/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/run-wa75hk1c.wandb (deflated 79%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/files/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/files/requirements.txt (deflated 55%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/files/wandb-summary.json (deflated 47%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/files/wandb-metadata.json (deflated 44%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/files/config.yaml (deflated 74%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/files/output.log (deflated 86%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/logs/ (stored 0%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/logs/debug-core.log (deflated 64%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/logs/debug-internal.log (deflated 74%)\n",
            "  adding: content/wandb/run-20241221_153508-wa75hk1c/logs/debug.log (deflated 68%)\n",
            "  adding: content/wandb/latest-run/run-wa75hk1c.wandb (deflated 79%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4daeea16-fecb-461d-b9b2-ba44914ccdda\", \"file.zip\", 4242063356)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!zip -r /content/file.zip /content\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab60c7cea29d462aa75bb730ce11f750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb6ee71b97e642558f7f6903d335285b",
              "IPY_MODEL_21039639d149416db4741b3cd54b7f27",
              "IPY_MODEL_2d328342e6664f7ca0ef31420bce4911"
            ],
            "layout": "IPY_MODEL_f18a1bbb76b049c2a1aa3e02be684369"
          }
        },
        "eb6ee71b97e642558f7f6903d335285b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3709d0c8e4241f89b9f411cfbe350e0",
            "placeholder": "​",
            "style": "IPY_MODEL_3e51d8a214ea4d9cac422c0b947042b0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "21039639d149416db4741b3cd54b7f27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962dd6e798c74355a092724b971b73f2",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57a7b7dc6d4f49038c74b1ddf1050ee9",
            "value": 8
          }
        },
        "2d328342e6664f7ca0ef31420bce4911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c57afe3f6a642efa2ac25487f94501f",
            "placeholder": "​",
            "style": "IPY_MODEL_02d84bf01bd84b299cca24b1c7b5dc41",
            "value": " 8/8 [00:06&lt;00:00,  1.28it/s]"
          }
        },
        "f18a1bbb76b049c2a1aa3e02be684369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3709d0c8e4241f89b9f411cfbe350e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e51d8a214ea4d9cac422c0b947042b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "962dd6e798c74355a092724b971b73f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57a7b7dc6d4f49038c74b1ddf1050ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c57afe3f6a642efa2ac25487f94501f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02d84bf01bd84b299cca24b1c7b5dc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "344c0d5411604acdb9c97b06efb0ffae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3b9d9c3a8b84ae29032f8245041fdb6",
              "IPY_MODEL_8247dada9c4a46b59dc7da9a34ef2261",
              "IPY_MODEL_c97eccca84d941648870bc0e4044e7da"
            ],
            "layout": "IPY_MODEL_bc3917063ce9468aa4adc34f5f0a02fd"
          }
        },
        "b3b9d9c3a8b84ae29032f8245041fdb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f71b86daf3c640588fdb0e05a789cab7",
            "placeholder": "​",
            "style": "IPY_MODEL_fd556626f812485e9b986236bc303e0f",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "8247dada9c4a46b59dc7da9a34ef2261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f015bc8e46343f3ab99878210048b18",
            "max": 27280152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f91942b04ef2457989c583ee99bccdea",
            "value": 27280152
          }
        },
        "c97eccca84d941648870bc0e4044e7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853aa6e74e5a491c8b7d612c100ec88d",
            "placeholder": "​",
            "style": "IPY_MODEL_412eba01a7fd406b9b42247fbd2cde40",
            "value": " 27.3M/27.3M [00:04&lt;00:00, 14.9MB/s]"
          }
        },
        "bc3917063ce9468aa4adc34f5f0a02fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f71b86daf3c640588fdb0e05a789cab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd556626f812485e9b986236bc303e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f015bc8e46343f3ab99878210048b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f91942b04ef2457989c583ee99bccdea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "853aa6e74e5a491c8b7d612c100ec88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "412eba01a7fd406b9b42247fbd2cde40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}